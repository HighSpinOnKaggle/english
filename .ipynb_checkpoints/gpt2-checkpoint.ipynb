{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a small GPT-2  like english model using trax framework\n",
    "\n",
    "Aim: train a small GPT-2 model english model. The model is trained on the CNN - cnn_dailymail database and  is case sensitive. The model can be used for the text summarization, sentiment analysis etc.\n",
    "\n",
    "This notebook is created by Vitaly Shklyar \n",
    "                        \n",
    "Transformer specification:\n",
    "* vocabulary size: 33300 words\n",
    "* embeding size  : 512 (default)\n",
    "* number of heads: 8    (default)\n",
    "* feedforward size: 2048 (default)\n",
    "* number of decoders: 6   (default)\n",
    "* position embedding length: 4096\n",
    "\n",
    "Hardware and framework version\n",
    "+ GTX 1080 Ti\n",
    "+ Ubuntu version 18.02\n",
    "+ trax version: 1.3.7\n",
    "+ python version: 3.6.9 \n",
    "+ cuda: 11.2\n",
    "+ nvidia driver version : 460.27.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_FLAGS=--xla_gpu_cuda_data_dir=/usr/local/cuda\n"
     ]
    }
   ],
   "source": [
    "%env XLA_FLAGS=--xla_gpu_cuda_data_dir=/usr/local/cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_PYTHON_CLIENT_PREALLOCATE=false\n"
     ]
    }
   ],
   "source": [
    "%env XLA_PYTHON_CLIENT_PREALLOCATE=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_PYTHON_CLIENT_ALLOCATOR=platform\n"
     ]
    }
   ],
   "source": [
    "%env XLA_PYTHON_CLIENT_ALLOCATOR=platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CChWzW-rEHVb",
    "outputId": "a0b3e98b-7fc6-492d-c8ad-3a263b54f670"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import textwrap\n",
    "wrapper = textwrap.TextWrapper(width=70)\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.fastmath import numpy as jnp\n",
    "from trax.supervised import training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kEL2rvaHRWP4"
   },
   "source": [
    "<a name='1'></a>\n",
    "## Function definintions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "djTiSLcaNFGa"
   },
   "outputs": [],
   "source": [
    "def tokenize(input_str):\n",
    "    \"\"\" Tokenizes input string for embedding.\n",
    "        Input: \n",
    "            input_str: (string) input string to tokenize\n",
    "        Return:        \n",
    "            list of integers for embedding.\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs =  next(trax.data.tokenize(iter([input_str]),\n",
    "                                      vocab_dir='vocab_dir/',\n",
    "                                      vocab_file='vocabulary_en'))\n",
    "    return list(inputs) \n",
    "\n",
    "def detokenize(integer_list):\n",
    "    \"\"\" List of integers to string.\n",
    "        Input: \n",
    "            integer_list: list of integers        \n",
    "        Return:\n",
    "            string of tokens        \n",
    "    \"\"\"  \n",
    "    s = trax.data.detokenize(integer_list,\n",
    "                             vocab_dir='vocab_dir/',\n",
    "                             vocab_file='vocabulary_en')    \n",
    "    return wrapper.fill(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_eos = np.array([EOS])\n",
    "def preprocess(stream):\n",
    "    for (feature, target) in stream:\n",
    "        new_feature = np.concatenate([feature, np_eos],axis =0)\n",
    "        yield new_feature,new_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_stream(train_stream, \n",
    "                 max_length = 2048, \n",
    "                 #boundaries = [64, 128, 512, 2048], \n",
    "                 #batch_sizes = [32,  16,   8,  2, 1 ]):\n",
    "                 boundaries = [64, 128, 512, 2048], \n",
    "                 batch_sizes = [64,  32,   16,  2, 1 ]):\n",
    "    \"\"\"\n",
    "    Create batch stream.\n",
    "        Input:\n",
    "            train_stream: (boolean) defines stream type.  True: returns train stream, False: evaluation stream.\n",
    "            max_length: (int) the maximum data lengt in the stream.\n",
    "            boundaries: list of integers for bucketing boundaries\n",
    "            batch_size: list of integers for the batch sizes corresponding bucketing boundaries\n",
    "    \"\"\"\n",
    "    if train_stream not in (True,False):\n",
    "        raise Exception(\"Wrong intput train_stream value. Allowed values are True or False\")\n",
    "    \n",
    "    return trax.data.Serial(\n",
    "    trax.data.TFDS('cnn_dailymail',\n",
    "                                 data_dir='data/',\n",
    "                                 keys=('article', 'article'),\n",
    "                                 train=train_stream),\n",
    "    trax.data.Tokenize(vocab_dir='vocab_dir/',\n",
    "                       vocab_file='vocabulary_en'),\n",
    "    preprocess, \n",
    "    trax.data.Shuffle(102400),\n",
    "    trax.data.FilterByLength(max_length=2048),\n",
    "    trax.data.BucketByLength(boundaries=boundaries, batch_sizes=batch_sizes),\n",
    "    trax.data.AddLossWeights(id_to_mask=0)\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gM2gpu4xvjtX"
   },
   "outputs": [],
   "source": [
    "def training_loop(model, train_gen, eval_gen, steps=50,  output_dir = \"model_gpt2/\"):\n",
    "\n",
    "    output_dir = os.path.expanduser(output_dir)  # trainer is an object\n",
    "    lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=1000, max_value=0.0025)\n",
    "\n",
    "\n",
    "    train_task = training.TrainTask( \n",
    "      labeled_data=train_gen(), \n",
    "      loss_layer=tl.WeightedCategoryCrossEntropy(),\n",
    "      optimizer=trax.optimizers.Adam(0.0025), \n",
    "      lr_schedule=lr_schedule,\n",
    "      n_steps_per_checkpoint=steps\n",
    "    )\n",
    "\n",
    "    eval_task = training.EvalTask( \n",
    "      labeled_data=eval_gen(), \n",
    "      metrics=[tl.WeightedCategoryCrossEntropy(), tl.WeightedCategoryAccuracy()] \n",
    "    )\n",
    "    \n",
    "    loop = training.Loop(model,\n",
    "                         train_task,\n",
    "                         eval_tasks=[eval_task],\n",
    "                         output_dir=output_dir)\n",
    "    return loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_token(input_tokens, model, temperature = 0.0):\n",
    "    \"\"\" Generate next token given the input sequence.\n",
    "\n",
    "    Input:\n",
    "        input_tokens (list): tokenized text as a list of integers\n",
    "        model: language model\n",
    "        temperature: (float) a small positive number to bring arbitrariness to the next token selection \n",
    "\n",
    "    Returns:\n",
    "        int: generated token\n",
    "    \"\"\"\n",
    "    \n",
    "    token_length = len(input_tokens)\n",
    "    padded_length = 2**int(np.ceil(np.log2(token_length + 1)))\n",
    "\n",
    "    # pad the input sequence to make total input equal 2**N where N is an integer  \n",
    "    padded = input_tokens + [0] * (padded_length - token_length)\n",
    "    padded_with_batch = np.array(padded)[None, :] \n",
    "\n",
    "    output, _ = model((padded_with_batch, padded_with_batch)) \n",
    "    # TransformerLM in trax version 1.3.7 has no softmax layer \n",
    "    log_probs = output[0, token_length, :]\n",
    "    log_probs = np.exp(log_probs)\n",
    "    log_probs /= np.sum(log_probs)\n",
    "    return tl.logsoftmax_sample(np.log(log_probs),temperature)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_text(input_text, model, temperature=0.0, max_length=256):\n",
    "    \"\"\" Create text.\n",
    "\n",
    "    Input:\n",
    "        input_text (string): a sentence or an article.\n",
    "        model: (TransformerLM) Transformer language model\n",
    "        temperature: (float) a small positive number to bring arbitrariness to the next token selection \n",
    "        max_length: (int) maximal length of the generated text\n",
    "\n",
    "    Returns:\n",
    "        generated text as a string\n",
    "    \"\"\"\n",
    "    input_tokens = tokenize(input_text)\n",
    "\n",
    "    generated_output = [] \n",
    "    new_token = 0 \n",
    "    \n",
    "    count=0\n",
    "    while new_token != EOS and count < max_length:\n",
    "        new_token = next_token(input_tokens, model,temperature=temperature)\n",
    "        input_tokens.append(new_token)\n",
    "        generated_output.append(new_token)       \n",
    "        count +=1\n",
    "    return detokenize(generated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trax.models.TransformerLM(  vocab_size=33300,\n",
    "                        dropout=0.1,\n",
    "                        max_len=4096,\n",
    "                        ff_activation=tl.Relu,\n",
    "                        mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "BFRBTwSqRWRZ",
    "outputId": "aff859e5-8f4a-4d3b-f1d3-98e137581a77"
   },
   "outputs": [],
   "source": [
    "loop = training_loop(model, batch_stream(train_stream=True), batch_stream(train_stream=False),1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step  637000: Ran 1000 train steps in 476.49 secs\n",
      "Step  637000: train WeightedCategoryCrossEntropy |  3.53724289\n",
      "Step  637000: eval  WeightedCategoryCrossEntropy |  3.59622145\n",
      "Step  637000: eval      WeightedCategoryAccuracy |  0.33544514\n"
     ]
    }
   ],
   "source": [
    "#loop.run(80000) \n",
    "loop.run(1000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del loop\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trax.models.TransformerLM(  vocab_size=33300,\n",
    "                        max_len=4096,\n",
    "                        ff_activation=tl.Relu,\n",
    "                        mode='eval')\n",
    "model.init_from_file('model_gpt2/model.pkl.gz', weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_sentence = \"Here comes the sun \"\n",
    "#s=next_token(tokenize(test_sentence), model, 0.8)\n",
    "#detokenize( [s] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate an arbitrary text using a starting sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Many people have experienced the Covid-19 pandemic as an event of pure\n",
      "novelty: a sudden and unexpected break from the past. \n",
      "\n",
      "But this is the 18th-century Chinese pandemic. The Chen Zhou daily\n",
      "ritual in Tianxi was known by young Chinese students for the ‘ٹev-\n",
      "McNair’, or what is the Chinese capital of China’s Sichuan Province.\n",
      "It was thought the Chinese government had an iron Yang-wound to the\n",
      "chest, but he suffered mild complications. Scroll down for video . The\n",
      "Chinese pandemic is also known by young Chinese students for the 'ổev-\n",
      "McNair' high-resolution mortally in Tianxi, China . There are now\n",
      "70,000 Chinese pandemic - originally named 'GA mainland' and was\n",
      "commissioned by China Public Library. The Xiang Liu Aug Xu, a\n",
      "spokesperson for Beijing both happens in recent years, said: ‘This\n",
      "shows a bit of pride in Beijing, but the body was like a joke.’ He\n",
      "added that China’s live ballistic nation are ‘as far as hell getting\n",
      "this flu’. And China, which was the beginning of Jordan in 2003, was\n",
      "ousted by Beijing, with at least 27 million Chinese officials and\n",
      "Chinese officials. China suspended three Chinese boys from their\n",
      "school. The ten\n"
     ]
    }
   ],
   "source": [
    "# Input text from the Gardian'a article\n",
    "#https://www.theguardian.com/commentisfree/2021/feb/22/hiv-covid-pandemics-fear-disease-prejudice\n",
    "\n",
    "input_sentence = \"Many people have experienced the Covid-19 pandemic as an event of pure novelty: a sudden and unexpected break from the past. \"\n",
    "print(wrapper.fill(input_sentence), '\\n')\n",
    "print(produce_text(input_sentence, model, temperature=0.9))"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "NLPC4-2"
   ]
  },
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "trax",
   "language": "python",
   "name": "trax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
