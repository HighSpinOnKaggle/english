{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Simple model for the disaster tweet analysis\n",
    "\n",
    "The aim is to get a maximum possible score of the classification of the disaster tweet without using any pretrained model.\n",
    "The best score in public notebooks is obtained so far by using a pretrained BERT model with the avarage score=0.84 \n",
    "\n",
    "To run this notebook load the following file from the Kaggle https://www.kaggle.com/c/nlp-getting-started \n",
    " - train.csv\n",
    " - test.csv\n",
    " - sample_submission.csv\n",
    " \n",
    "The best public test score for this model is 0.7961\n",
    "\n",
    "\n",
    "\n",
    "This notebook is created by Vitaly Shklyar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn import feature_extraction\n",
    "from sklearn.utils import shuffle\n",
    "from nltk import FreqDist \n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import GRU,LSTM,Embedding, Input, Dense\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "from tensorflow.keras import regularizers, callbacks\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.regularizers import  l2,l1,Regularizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_words(df_text):\n",
    "    words = []\n",
    "    for s in df_text:\n",
    "        words += s.split() \n",
    "    return set(words)\n",
    "\n",
    "def keep_words(sentence, words_list ):\n",
    "    words = sentence.split()\n",
    "    return ' '.join([w for w in words if w in words_list])\n",
    "\n",
    "# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def find_emoji(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE)\n",
    "    print( emoji_pattern.findall(r'', text))\n",
    "    \n",
    "def lowerer(t):\n",
    "    return t.lower()    \n",
    "\n",
    "def remove_hyperlins(text):\n",
    "    res = re.sub(r'(\\S{0,}http\\S{0,})+',' ',text.lower())\n",
    "    return res\n",
    "\n",
    "def keep_letters_only(text):\n",
    "    return ' '.join(re.findall(r'[a-zA-Z]{3,}',text))\n",
    "\n",
    "def remove_ets(text):\n",
    "    res = re.sub(r'(\\S{0,}@\\S{0,})+',' ',text.lower())\n",
    "    return res\n",
    "\n",
    "def decontracted(phrase):\n",
    "    \"\"\"Convert contractions like \"can't\" into \"can not\"\n",
    "    \"\"\"\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    #phrase = re.sub(r\"n't\", \" not\", phrase) # resulted in \"ca not\" when sentence started with \"can't\"\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "def clean(tweet):\n",
    "    \"\"\"\n",
    "    substitute special charackters and number by empty spaces; may be it is not good since the numbers could be inromative \n",
    "    \"\"\"\n",
    "    # Punctuations at the start or end of words    \n",
    "    for punctuation in \"!@#$%^&*()_{}[]+?\"\"\\':;,./-+=0123456789\\n\\\\\":\n",
    "        tweet = tweet.replace(punctuation, f' ')\n",
    "    return tweet.strip()\n",
    "\n",
    "def remove_words(sentence, words_list ):\n",
    "    words = sentence.split()\n",
    "    return ' '.join([w for w in words if w not in words_list])\n",
    "\n",
    "def unitque_words_count(df_text):\n",
    "    words = []\n",
    "    for s in df_text:\n",
    "        words += s.split() \n",
    "    return len(set(words))\n",
    "\n",
    "def prepare_tockens(tockenizer, text):\n",
    "    tensor = tockenizer.texts_to_sequences(text)\n",
    "    tensor = pad_sequences(tensor,maxlen=50)\n",
    "    (n,m ) = tensor.shape\n",
    "    return tensor.reshape((n,m))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the training set and shuffle it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "train_df  = shuffle(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Remove emoji "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['no_emoji'] = train_df['text'].apply(lambda  x: remove_emoji(x))\n",
    "test_df['no_emoji'] = test_df['text'].apply(lambda  x: remove_emoji(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Lower the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text_lower'] = train_df['no_emoji'].apply(lambda  x: lowerer(x))\n",
    "test_df['text_lower'] = test_df['no_emoji'].apply(lambda  x: lowerer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Remove hyperlinks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['no_hyperlinks'] = train_df['text_lower'].apply(lambda  x: remove_hyperlins(x))\n",
    "test_df['no_hyperlinks'] = test_df['text_lower'].apply(lambda  x: remove_hyperlins(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['no_ets'] = train_df['no_hyperlinks'].apply(lambda  x: remove_ets(x))\n",
    "test_df['no_ets'] = test_df['no_hyperlinks'].apply(lambda  x: remove_ets(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Decontract "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"decontracted\"] = train_df[\"no_ets\"].apply(lambda x: decontracted(x))\n",
    "test_df[\"decontracted\"]  = test_df[\"no_ets\"].apply(lambda x: decontracted(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Remove special characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['alpha'] = train_df['decontracted'].apply(lambda  x: clean(x))\n",
    "test_df['alpha'] = test_df['decontracted'].apply(lambda  x: clean(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Find the rare words in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique words: 14493 \n",
      "number of rare words which appear less then 2 times, count = 10148\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "word_frequencsy=2\n",
    "for s in  train_df[\"alpha\"]:\n",
    "    words+=s.split()\n",
    "print(f\"unique words: {len(set(words) ) } \")\n",
    "\n",
    "fdist1 = FreqDist(words)\n",
    "\n",
    "rare_words = [w for w in set(words) if fdist1[w] <= word_frequencsy]\n",
    "print(f\"number of rare words which appear less then {word_frequencsy} times, count = {len(rare_words)}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Concatenate the rare and unique words and remove then from the training and the test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove=feature_extraction.text.ENGLISH_STOP_WORDS.union(rare_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text_no_rare_words']      = train_df['alpha'].apply(lambda x : remove_words(x,words_to_remove))\n",
    "test_df['text_no_rare_words']       = test_df['alpha'].apply(lambda x : remove_words(x,words_to_remove))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Check the number of unique words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4094"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unitque_words_count(train_df['text_no_rare_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6634"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unitque_words_count(test_df['text_no_rare_words'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Keep words with length => 3 and consisting only from letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"cleared_letters_only\"] = train_df[\"text_no_rare_words\"].apply(lambda x:keep_letters_only(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"cleared_letters_only\"] = test_df[\"text_no_rare_words\"].apply(lambda x:keep_letters_only(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3897"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unitque_words_count(train_df[\"cleared_letters_only\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6356"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unitque_words_count(test_df[\"cleared_letters_only\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Tockenize for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_txt      = train_df[\"cleared_letters_only\"]\n",
    "test_txt       = test_df[\"cleared_letters_only\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_tokenizer = Tokenizer( filters='')\n",
    "lang_tokenizer.fit_on_texts(train_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_np = prepare_tockens(lang_tokenizer, train_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Create and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(4000, 64, embeddings_regularizer=regularizers.l2(0.0000003) ))\n",
    "model.add(GRU(16, recurrent_regularizer=regularizers.l2(0.00001) ) )\n",
    "model.add(Dense(1, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.3092 - acc: 0.8842 - val_loss: 0.4825 - val_acc: 0.7892\n",
      "Epoch 2/10000\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2902 - acc: 0.8928 - val_loss: 0.4924 - val_acc: 0.7886\n",
      "Epoch 3/10000\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2757 - acc: 0.8990 - val_loss: 0.5027 - val_acc: 0.7827\n",
      "Epoch 4/10000\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2631 - acc: 0.9039 - val_loss: 0.5148 - val_acc: 0.7774\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00004: early stopping\n"
     ]
    }
   ],
   "source": [
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=3, restore_best_weights=True, verbose=1)\n",
    "history = model.fit(train_np, train_df[\"target\"].values, batch_size =16 , epochs =100, validation_split=0.2, shuffle = True, callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 60)          240000    \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 10)                2160      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 242,171\n",
      "Trainable params: 242,171\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Keep only known words in the test set for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_keep = unique_words(train_df[\"cleared_letters_only\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['cleared_letters_only'] = test_df['cleared_letters_only'].apply(lambda x : keep_words(x,words_to_keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3180"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unitque_words_count(test_df[\"cleared_letters_only\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_txt = test_df[\"cleared_letters_only\"]\n",
    "test_np  = prepare_tockens(lang_tokenizer, test_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(x=test_np)\n",
    "preds[preds>0.5] = 1\n",
    "preds[preds<0.5] = 0\n",
    "preds=preds.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
    "sample_submission['target'] = preds.astype(int)\n",
    "sample_submission.to_csv(\"SimpleRNNnoPretraining.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
