{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a small GPT-2  like english model for text summarization using trax framework\n",
    "\n",
    "Aim: apply GPT-2 model for the  text summarization. The model is trained on the CNN - cnn_dailymail database and is case sensitive.\n",
    "\n",
    "This notebook is created by Vitaly Shklyar \n",
    "\n",
    "\n",
    "                        \n",
    "Transformer specification:\n",
    "* vocabulary size: 33300 words\n",
    "* embeding size  : 512 (default)\n",
    "* number of heads: 8   (default)\n",
    "* feedforward size: 2024 (default)\n",
    "* number of decoders: 6  (default)\n",
    "* position embedding length: 4096  (default)\n",
    "\n",
    "Hardware and framework version\n",
    "+ GTX 1080 Ti\n",
    "+ Ubuntu version 18.02\n",
    "+ trax version: 1.3.7\n",
    "+ python version: 3.6.9 \n",
    "+ cuda: 11.2\n",
    "+ nvidia driver version : 460.27.04\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_FLAGS=--xla_gpu_cuda_data_dir=/usr/local/cuda\n"
     ]
    }
   ],
   "source": [
    "%env XLA_FLAGS=--xla_gpu_cuda_data_dir=/usr/local/cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_PYTHON_CLIENT_PREALLOCATE=false\n"
     ]
    }
   ],
   "source": [
    "%env XLA_PYTHON_CLIENT_PREALLOCATE=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_PYTHON_CLIENT_ALLOCATOR=platform\n"
     ]
    }
   ],
   "source": [
    "%env XLA_PYTHON_CLIENT_ALLOCATOR=platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS = 1\n",
    "PAD = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CChWzW-rEHVb",
    "outputId": "a0b3e98b-7fc6-492d-c8ad-3a263b54f670"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import textwrap\n",
    "wrapper = textwrap.TextWrapper(width=70)\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.fastmath import numpy as jnp\n",
    "from trax.supervised import training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kEL2rvaHRWP4"
   },
   "source": [
    "<a name='1'></a>\n",
    "## Function definintions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "djTiSLcaNFGa"
   },
   "outputs": [],
   "source": [
    "def tokenize(input_str):\n",
    "    \"\"\" Tokenizes input string for embedding.\n",
    "        Input: \n",
    "            input_str: (string) input string to tokenize\n",
    "        Return:        \n",
    "            list of integers for embedding.\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs =  next(trax.data.tokenize(iter([input_str]),\n",
    "                                      vocab_dir='vocab_dir/',\n",
    "                                      vocab_file='vocabulary_en'))\n",
    "    return list(inputs) \n",
    "\n",
    "def detokenize(integer_list):\n",
    "    \"\"\" List of integers to string.\n",
    "        Input: \n",
    "            integer_list: list of integers        \n",
    "        Return:\n",
    "            string of tokens        \n",
    "    \"\"\"  \n",
    "    s = trax.data.detokenize(integer_list,\n",
    "                             vocab_dir='vocab_dir/',\n",
    "                             vocab_file='vocabulary_en')    \n",
    "    return wrapper.fill(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_eos = np.array([EOS])\n",
    "np_eos_pad = np.array([EOS,PAD])\n",
    "        \n",
    "def preprocess(stream):\n",
    "    '''\n",
    "    Function to input stream for the text summarization: concatenates the article and the summary and \n",
    "    creates the training mask. The article and summary are concatened in the form: \n",
    "    article + EOS + PAD + summary + EOS\n",
    "    \n",
    "    the training mask is created in such a way that only summary and the last EOS token contribute \n",
    "    to the cost function. \n",
    "    \n",
    "    Input:\n",
    "        stream: (article, summary) stream containing a tokenized article and a summary \n",
    "        \n",
    "    Returns:\n",
    "        (train_stream, train_target, mask)\n",
    "    '''\n",
    "    for (article, summary) in stream:\n",
    "        a = np.concatenate([article, np_eos_pad, summary, np_eos],axis =0)\n",
    "        m = np.ones(len(a))\n",
    "        m[:len(article)+2]=0\n",
    "        yield a,a,m        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_stream(train_stream, \n",
    "                 max_length = 2048, \n",
    "#                 boundaries = [64, 128, 512, 2048], \n",
    "#                 batch_sizes = [64,  32,   16,  4, 1 ],\n",
    "                 boundaries  = [64, 128, 512, 2048], \n",
    "                 batch_sizes = [64,  32,   16,  2, 1 ]):\n",
    "    \"\"\"\n",
    "    Create batch stream.\n",
    "        Input:\n",
    "            train_stream: (boolean) defines stream type.  True: returns train stream, False: evaluation stream.\n",
    "            max_length: (int) the maximum data lengt in the stream.\n",
    "            boundaries: list of integers for bucketing boundaries\n",
    "            batch_size: list of integers for the batch sizes corresponding bucketing boundaries\n",
    "    \"\"\"\n",
    "    if train_stream not in (True,False):\n",
    "        raise Exception(\"Wrong intput train_stream value. Allowed values are True or False\")\n",
    "    \n",
    "    return trax.data.Serial(\n",
    "    trax.data.TFDS('cnn_dailymail',\n",
    "                                 data_dir='data/',\n",
    "                                 keys=('article', 'highlights'),\n",
    "                                 train=train_stream),\n",
    "    trax.data.Tokenize(vocab_dir='vocab_dir/',\n",
    "                       vocab_file='vocabulary_en'),\n",
    "    preprocess, \n",
    "    trax.data.Shuffle(102400),\n",
    "    trax.data.FilterByLength(max_length=2048),\n",
    "    trax.data.BucketByLength(boundaries=boundaries, batch_sizes=batch_sizes),\n",
    "    trax.data.AddLossWeights(id_to_mask=0)\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gM2gpu4xvjtX"
   },
   "outputs": [],
   "source": [
    "def training_loop(model, train_gen, eval_gen, steps=50,  output_dir = \"model_gpt2_summarization/\"):\n",
    "\n",
    "    output_dir = os.path.expanduser(output_dir)  # trainer is an object\n",
    "    lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=1000, max_value=0.002)\n",
    "\n",
    "\n",
    "    train_task = training.TrainTask( \n",
    "      labeled_data=train_gen(), \n",
    "      loss_layer=tl.WeightedCategoryCrossEntropy(),\n",
    "      optimizer=trax.optimizers.Adam(0.002), \n",
    "      lr_schedule=lr_schedule,\n",
    "      n_steps_per_checkpoint=steps\n",
    "    )\n",
    "\n",
    "    eval_task = training.EvalTask( \n",
    "      labeled_data=eval_gen(), \n",
    "      metrics=[tl.WeightedCategoryCrossEntropy(), tl.WeightedCategoryAccuracy()] \n",
    "    )\n",
    "    \n",
    "    loop = training.Loop(model,\n",
    "                         train_task,\n",
    "                         eval_tasks=[eval_task],\n",
    "                         output_dir=output_dir)\n",
    "    return loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_random_token(input_tokens, model, temperature = 0.0):\n",
    "    \"\"\" Generate next token given the input sequence.\n",
    "\n",
    "    Input:\n",
    "        input_tokens (list): tokenized text as a list of integers\n",
    "        model: language model\n",
    "        temperature: (float) a small positive number to bring arbitrariness to the next token selection \n",
    "\n",
    "    Returns:\n",
    "        int: generated token\n",
    "    \"\"\"\n",
    "    \n",
    "    token_length = len(input_tokens)\n",
    "    padded_length = 2**int(np.ceil(np.log2(token_length + 1)))\n",
    "\n",
    "    # pad the input sequence to make total input equal 2**N where N is an integer  \n",
    "    padded = input_tokens + [0] * (padded_length - token_length)\n",
    "    padded_with_batch = np.array(padded)[None, :] \n",
    "\n",
    "    output, _ = model((padded_with_batch, padded_with_batch)) \n",
    "    # TransformerLM in trax version 1.3.7 has no softmax layer \n",
    "    log_probs = output[0, token_length, :]\n",
    "    log_probs = np.exp(log_probs)\n",
    "    log_probs /= np.sum(log_probs)\n",
    "    return tl.logsoftmax_sample(np.log(log_probs),temperature)\n",
    "\n",
    "\n",
    "\n",
    "def get_summary(input_text, model, temperature=0.0, max_length=256):\n",
    "    \"\"\" Create text.\n",
    "\n",
    "    Input:\n",
    "        input_text (string): a sentence or an article.\n",
    "        model: (TransformerLM) Transformer language model\n",
    "        temperature: (float) a small positive number to bring arbitrariness to the next token selection \n",
    "        max_length: (int) maximal length of the generated text\n",
    "\n",
    "    Returns:\n",
    "        generated text as a string\n",
    "    \"\"\"\n",
    "    input_tokens = tokenize(input_text)+ [EOS]+ [PAD]\n",
    "\n",
    "    generated_output = [] \n",
    "    new_token = 0 \n",
    "    \n",
    "    count=0\n",
    "    while new_token != EOS and count < max_length:\n",
    "        new_token = next_random_token(input_tokens, model,temperature=temperature)\n",
    "        input_tokens.append(new_token)\n",
    "        generated_output.append(new_token)       \n",
    "        count +=1\n",
    "    return detokenize(generated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trax.models.TransformerLM(  vocab_size=33300,\n",
    "                        dropout=0.1,\n",
    "                        max_len=4096,\n",
    "                        ff_activation=tl.Relu,\n",
    "                        mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "BFRBTwSqRWRZ",
    "outputId": "aff859e5-8f4a-4d3b-f1d3-98e137581a77"
   },
   "outputs": [],
   "source": [
    "loop = training_loop(model, batch_stream(train_stream=True), batch_stream(train_stream=False),1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step  797000: Ran 1000 train steps in 485.41 secs\n",
      "Step  797000: train WeightedCategoryCrossEntropy |  2.63424301\n",
      "Step  797000: eval  WeightedCategoryCrossEntropy |  3.19755816\n",
      "Step  797000: eval      WeightedCategoryAccuracy |  0.49704143\n"
     ]
    }
   ],
   "source": [
    "#loop.run(80000)  \n",
    "loop.run(1000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del loop\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trax.models.TransformerLM(  vocab_size=33300,\n",
    "                        max_len=4096,\n",
    "                        ff_activation=tl.Relu,\n",
    "                        mode='eval')\n",
    "# Load trained weights\n",
    "model.init_from_file('model_gpt2_summarization/model.pkl.gz', weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get summary for the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millions of television viewers around the world are in a state of\n",
      "frenzied anticipation on Sunday, as they await the broadcast of Oprah\n",
      "Winfrey's set-piece interview with Prince Harry and Meghan, Duchess of\n",
      "Sussex. The primetime event, which will be shown on Sunday evening in\n",
      "the United States, has been relentlessly promoted by network CBS and\n",
      "threatens to lift the lid on a litany of frustrations and grievances\n",
      "held by the couple against the institution they quit last year.I don't\n",
      "know how they could expect that after all of this time, we would still\n",
      "just be silent if there is an active role that The Firm is playing in\n",
      "perpetuating falsehoods about us, Meghan said in a clip already\n",
      "released, hinting that she is ready to escalate a war of words between\n",
      "herself and the family she married into. \n",
      "\n",
      "Summary\n",
      "Oprah Winfrey's set-piece interview with Prince Harry and Meghan\n",
      "Duchess of Sussex . The pair are in a state of frenzied ffron Sunday,\n",
      "as they await the broadcast of Oprah Winfrey's set-piece interview .\n",
      "Winfrey's set-piece interview with Prince Harry and Meghan .<EOS>\n"
     ]
    }
   ],
   "source": [
    "# original text from cnn\n",
    "#https://edition.cnn.com/2021/03/07/world/harry-meghan-oprah-interview-preview-scli-gbr-intl/index.html\n",
    "input_sentence =\"Millions of television viewers around the world are in a state of frenzied anticipation on Sunday, as they await the broadcast of Oprah Winfrey's set-piece interview with Prince Harry and Meghan, Duchess of Sussex. The primetime event, which will be shown on Sunday evening in the United States, has been relentlessly promoted by network CBS and threatens to lift the lid on a litany of frustrations and grievances held by the couple against the institution they quit last year.I don't know how they could expect that after all of this time, we would still just be silent if there is an active role that The Firm is playing in perpetuating falsehoods about us, Meghan said in a clip already released, hinting that she is ready to escalate a war of words between herself and the family she married into.\"\n",
    "print(wrapper.fill(input_sentence), '\\n')\n",
    "\n",
    "print(\"Summary:\")\n",
    "generated_text = get_summary(input_sentence , model, temperature=0.2)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary.\n",
    "* The model is based on a quite small language model trained on a small corpus (3Gb)\n",
    "* The major shotcoming is that it ofen produces many repeating sentences\n",
    "* introducing a small temperature helps to avoid the \n",
    "\n",
    "### How to improve:\n",
    "* scale the model\n",
    "* train  the language model on a larger corpus\n",
    "* use mbr or beam search for sammarization (will slow down the calcuations)"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "NLPC4-2"
   ]
  },
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "trax",
   "language": "python",
   "name": "trax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
